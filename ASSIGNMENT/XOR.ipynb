{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxI_LoRgbFko",
        "vscode": {
          "languageId": "latex"
        }
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "    &\\textbf{1: Load necessary libraries}\\\\\n",
        "    &\\textbf{2: Load Dataset}\\\\\n",
        "    &\\textbf{2.1: Read in dataset from location}\\\\\n",
        "    &\\textbf{2.2: Format dataset into tensors}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DgpTS2rzjCS6"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CXEXUNbYjKdi"
      },
      "outputs": [],
      "source": [
        "# create data\n",
        "# Input data. Next to the input is the actual output in a comment\n",
        "x_train = torch.Tensor([[0., 0.],  # 0\n",
        "                        [0., 1.],  # 1\n",
        "                        [1., 0.],  # 1\n",
        "                        [1., 1.]])  # 0\n",
        "\n",
        "# Output data\n",
        "y_train = torch.Tensor([0., 1., 1., 0.]).reshape(x_train.shape[0], 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "    \\textbf{------------------------- GPU STUFF BEGIN -------------------------}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "78gZ9FrXaxno"
      },
      "outputs": [],
      "source": [
        "# Neural Network:\n",
        "dim_input  = 2\n",
        "dim_output = 1\n",
        "\n",
        "model = nn.Sequential(nn.Linear(dim_input, 10),  # HL1 (input)\n",
        "                      nn.ReLU(),                 # Activation function\n",
        "\n",
        "                      nn.Linear(10,10),          # HL1 -> HL2\n",
        "                      nn.ReLU(),                 # Activation function\n",
        "\n",
        "                      nn.Linear(10,10),          # HL2 -> HL3\n",
        "                      nn.ReLU(),                 # Activation function\n",
        "\n",
        "                      nn.Linear(10, dim_output)  # HL3 -> OUTPUT\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FABXFCrRNm_O",
        "outputId": "b734a22b-71c2-4ea3-878b-c46843eac25d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HYPER PARAMETERS & PARAMETERS\n",
        "\n",
        "# The learning rate\n",
        "learning_rate = 0.05\n",
        "# The loss function\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "# The optimizer (E.G.) What function we use to optimize our model\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# This is used to intialized the weights within our model, when nn.apply(this_fn)\n",
        "# is called our, all the children from nn.children have this function applied\n",
        "def init_normal(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "model.apply(init_normal)\n",
        "\n",
        "# named_layers = dict(model.named_modules())\n",
        "\n",
        "# print(f\"{named_layers}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bBfJBaGka2go"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "999 1.0\n",
            "1999 1.0\n",
            "2999 1.0\n",
            "3999 1.0\n",
            "4999 1.0\n",
            "5999 1.0\n",
            "6999 1.0\n",
            "7999 1.0\n",
            "8999 1.0\n",
            "9999 1.0\n",
            "10999 1.0\n",
            "11999 1.0\n",
            "12999 0.9999999403953552\n",
            "13999 1.0\n",
            "14999 1.0\n",
            "15999 1.0\n",
            "16999 0.9999999403953552\n",
            "17999 0.9999999403953552\n",
            "18999 1.0\n",
            "19999 1.0\n",
            "20999 0.9999999403953552\n",
            "21999 1.0\n",
            "22999 1.0\n",
            "23999 1.0\n",
            "24999 1.0\n",
            "25999 1.0\n",
            "26999 1.0\n",
            "27999 1.0\n",
            "28999 1.0\n",
            "29999 0.9999999403953552\n",
            "30999 0.9999999403953552\n",
            "31999 0.9999999403953552\n",
            "32999 0.9999999403953552\n",
            "33999 0.9999999403953552\n",
            "34999 0.9999999403953552\n",
            "35999 0.9999999403953552\n",
            "36999 0.9999998807907104\n",
            "37999 0.9999998807907104\n",
            "38999 0.9999998807907104\n",
            "39999 0.9999998807907104\n",
            "40999 0.9999998211860657\n",
            "41999 0.9999999403953552\n",
            "42999 0.9999998807907104\n",
            "43999 0.9999998807907104\n",
            "44999 0.9999998211860657\n",
            "45999 0.9999998211860657\n",
            "46999 0.9999998211860657\n",
            "47999 0.9999997615814209\n",
            "48999 0.9999997615814209\n",
            "49999 0.9999997615814209\n",
            "50999 0.9999997615814209\n",
            "51999 0.9999997019767761\n",
            "52999 0.9999996423721313\n",
            "53999 0.9999995231628418\n",
            "54999 0.9999995231628418\n",
            "55999 0.999999463558197\n",
            "56999 0.9999993443489075\n",
            "57999 0.9999992847442627\n",
            "58999 0.9999992251396179\n",
            "59999 0.9999991655349731\n",
            "60999 0.999998927116394\n",
            "61999 0.9999987483024597\n",
            "62999 0.9999984502792358\n",
            "63999 0.9999980926513672\n",
            "64999 0.9999974966049194\n",
            "65999 0.9999966621398926\n",
            "66999 0.9999953508377075\n",
            "67999 0.9999932050704956\n",
            "68999 0.9999889731407166\n",
            "69999 0.9999788999557495\n",
            "70999 0.9999441504478455\n",
            "71999 0.9996218681335449\n",
            "72999 4.358954583949526e-07\n",
            "73999 3.4894168265964254e-07\n",
            "74999 2.796676028538059e-07\n",
            "75999 2.2443194325205695e-07\n",
            "76999 1.8033591686617e-07\n",
            "77999 1.449273128173445e-07\n",
            "78999 1.1652632281311526e-07\n",
            "79999 9.375460052751805e-08\n",
            "80999 7.549455460775789e-08\n",
            "81999 6.080739467506646e-08\n",
            "82999 4.898670979969211e-08\n",
            "83999 3.9720656985764435e-08\n",
            "84999 3.224798206247215e-08\n",
            "85999 2.6180970280620386e-08\n",
            "86999 2.125107911865598e-08\n",
            "87999 1.7257484330457373e-08\n",
            "88999 1.401814131440915e-08\n",
            "89999 1.1393301413420431e-08\n",
            "90999 9.2600487278105e-09\n",
            "91999 7.549457059496945e-09\n",
            "92999 6.279895714556005e-09\n",
            "93999 5.216626242088296e-09\n",
            "94999 4.3383319159318035e-09\n",
            "95999 3.6062532959846294e-09\n",
            "96999 3.008000737381167e-09\n",
            "97999 2.507980934041143e-09\n",
            "98999 2.0960859714591606e-09\n",
            "99999 1.761329970761949e-09\n",
            "100999 1.4820977778384758e-09\n",
            "101999 1.250799019913984e-09\n",
            "102999 1.0589329413335236e-09\n",
            "103999 9.851477411615406e-10\n",
            "104999 9.195955108509679e-10\n",
            "105999 8.577961674305357e-10\n",
            "106999 8.009239937933899e-10\n",
            "107999 7.475134955470253e-10\n",
            "108999 6.982167066738043e-10\n",
            "109999 6.519825790363143e-10\n",
            "110999 6.092223392428764e-10\n",
            "111999 5.693401861073255e-10\n",
            "112999 5.320285323406893e-10\n",
            "113999 4.973546574582599e-10\n",
            "114999 4.648836871012918e-10\n",
            "115999 4.3482295541963367e-10\n",
            "116999 4.064192316022286e-10\n",
            "117999 3.805225579078808e-10\n",
            "118999 3.5602731873751736e-10\n",
            "119999 3.3309188740560103e-10\n",
            "120999 3.1184205218082184e-10\n",
            "121999 2.92018853542686e-10\n",
            "122999 2.7344121411587707e-10\n",
            "123999 2.559470968499511e-10\n",
            "124999 2.397653742214345e-10\n",
            "125999 2.249934960563138e-10\n",
            "126999 2.1107278425080978e-10\n",
            "127999 1.9760351688269395e-10\n",
            "128999 1.8533022339006777e-10\n",
            "129999 1.7414393538306427e-10\n",
            "130999 1.630383189565876e-10\n",
            "131999 1.530251758641299e-10\n",
            "132999 1.4385216628998165e-10\n",
            "133999 1.3528407560858824e-10\n",
            "134999 1.2705310414862225e-10\n",
            "135999 1.196845539341851e-10\n",
            "136999 1.1225323448549318e-10\n",
            "137999 1.0523307225618339e-10\n"
          ]
        }
      ],
      "source": [
        "# FORWARD PROPAGATION:\n",
        "\n",
        "for t in range(200000):\n",
        "    # Calculates the output (the y prediction), this propagates the data through our model\n",
        "    y_pred = model(x_train)\n",
        "    # Calculates the loss using our defined function loss_fn = Mean Squared Error\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "    if t % 1000 == 999:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # if our targeted loss value is below some delta then we break as our model is 'complete' I.E. done training\n",
        "    # although we don't know how well our model actually works, could very well be overfit, not work on test data etc\n",
        "    if abs(loss) < 1e-10:\n",
        "        break\n",
        "\n",
        "    optimizer.zero_grad()  # sets the gradients to 0 before running the backward pass\n",
        "    loss.backward()       # the backward pass\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFrfbcmfSb5I",
        "outputId": "0c32b8e8-2c7f-484d-adbe-35f2a2533860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST INPUT AND OUTPUTS\n",
            "\n",
            "INPUT:           tensor([0., 0.])\n",
            "OUTPUT:          tensor([7.1190e-06])\n",
            "EXPECTED OUTPUT: tensor([0.])\n",
            "*****************\n",
            "INPUT:           tensor([0., 1.])\n",
            "OUTPUT:          tensor([1.])\n",
            "EXPECTED OUTPUT: tensor([1.])\n",
            "*****************\n",
            "INPUT:           tensor([1., 0.])\n",
            "OUTPUT:          tensor([1.])\n",
            "EXPECTED OUTPUT: tensor([1.])\n",
            "*****************\n",
            "INPUT:           tensor([1., 1.])\n",
            "OUTPUT:          tensor([-7.0222e-06])\n",
            "EXPECTED OUTPUT: tensor([0.])\n",
            "*****************\n"
          ]
        }
      ],
      "source": [
        "# TESTING Training Data\n",
        "\n",
        "# Looking at the output of our model\n",
        "# Here we propagate forward our data and get a y predicted value\n",
        "\n",
        "print(\"TEST INPUT AND OUTPUTS\\n\")\n",
        "for i,data in enumerate(x_train):\n",
        "    y_out = model(data)\n",
        "    print(f\"INPUT:           {data}\")\n",
        "    print(f\"OUTPUT:          {y_out.data}\")\n",
        "    print(f\"EXPECTED OUTPUT: {y_train[i]}\")\n",
        "    print(\"*****************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPEJgb08SDhc",
        "outputId": "29ccae9a-362b-4b07-a576-a2a338dc4ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.weight tensor([[-0.0067,  0.0368],\n",
            "        [ 0.0713, -0.0723],\n",
            "        [ 0.3800,  0.1526],\n",
            "        [ 0.4153, -0.4118],\n",
            "        [ 0.6244, -0.6194],\n",
            "        [ 0.0445, -0.0450],\n",
            "        [-0.5899,  0.7333],\n",
            "        [-0.0398,  0.0899],\n",
            "        [ 0.1429,  0.0515],\n",
            "        [ 0.0029, -0.0237]])\n",
            "0.bias tensor([-2.5033e-06, -2.1187e-03, -1.7685e-01, -3.7735e-03, -6.8647e-03,\n",
            "        -6.4822e-04,  2.6566e-09,  3.4920e-10, -7.2259e-02, -2.9091e-03])\n",
            "2.weight tensor([[-1.9690e-02,  6.8175e-03,  1.2018e-03, -1.4364e-02,  6.3322e-04,\n",
            "          4.7306e-03, -1.3504e-03, -1.6164e-04, -3.1412e-03, -1.1250e-02],\n",
            "        [ 2.7424e-02,  5.5043e-02, -2.7445e-01,  3.7794e-01,  5.5608e-01,\n",
            "          1.2650e-02,  6.0597e-01,  6.1696e-02, -9.6136e-02, -8.9583e-03],\n",
            "        [ 1.1469e-02,  4.7591e-02, -1.7515e-01,  2.1405e-01,  3.6802e-01,\n",
            "          2.0722e-02,  3.9708e-01,  3.7993e-02, -5.0554e-02,  2.3663e-03],\n",
            "        [ 2.9068e-02,  5.5069e-02, -1.9417e-01,  3.0798e-01,  4.3575e-01,\n",
            "          5.7007e-02,  4.4387e-01,  4.4569e-02, -9.2068e-02, -8.9008e-03],\n",
            "        [-1.2893e-02, -5.4072e-03,  8.1853e-03, -1.1098e-02, -1.2369e-02,\n",
            "         -1.0981e-02, -1.1534e-02, -1.9245e-03,  1.3890e-02, -2.4660e-03],\n",
            "        [ 1.4454e-02,  6.7333e-03,  2.0618e-02,  5.1238e-03, -4.8345e-03,\n",
            "          1.0661e-03,  1.5655e-02,  7.9678e-03, -3.6259e-04,  4.3176e-04],\n",
            "        [-1.6971e-02, -6.2185e-03,  7.7178e-03, -1.3178e-02,  3.8287e-03,\n",
            "         -1.1344e-04, -3.7928e-03, -2.1670e-02,  3.5531e-03, -1.1294e-02],\n",
            "        [-1.7460e-02,  1.5858e-02, -5.1438e-02,  8.0675e-02,  1.5369e-01,\n",
            "          1.4689e-02,  1.3185e-01,  2.0358e-02, -2.3736e-02,  1.0611e-02],\n",
            "        [-8.4008e-03,  3.0230e-02, -1.0760e-01,  1.4302e-01,  2.0839e-01,\n",
            "          4.0746e-03,  2.3868e-01,  3.6719e-02, -3.8346e-02, -7.4783e-03],\n",
            "        [ 7.0622e-03, -1.3110e-02, -4.1099e-03, -1.3733e-02, -6.0525e-03,\n",
            "         -1.1671e-03, -1.2874e-02, -1.0838e-02,  4.2692e-03, -1.0462e-03]])\n",
            "2.bias tensor([-6.1752e-05, -7.5487e-09,  1.9225e-09,  1.0659e-03,  8.7612e-03,\n",
            "        -8.4585e-03, -1.5435e-03,  1.5757e-09,  1.7134e-04, -2.0221e-04])\n",
            "4.weight tensor([[-1.3180e-02, -1.6093e-03,  2.1980e-02, -6.4169e-03,  9.1235e-03,\n",
            "          1.0401e-02, -1.0534e-02,  9.7464e-03, -2.0100e-02,  6.9448e-03],\n",
            "        [-8.6769e-03, -4.3167e-03,  3.2774e-03, -2.2209e-03, -2.1788e-04,\n",
            "          2.1904e-03, -5.2700e-03,  8.7302e-03,  1.5742e-02,  7.2141e-03],\n",
            "        [ 4.6145e-03, -6.5904e-03, -8.9010e-05, -1.3812e-02, -1.3162e-02,\n",
            "          1.4290e-03, -1.2392e-03, -2.3874e-02,  1.1392e-03,  5.6258e-03],\n",
            "        [-1.6217e-02, -1.5926e-01, -9.2298e-02, -1.0353e-01, -2.0833e-02,\n",
            "         -6.2844e-03, -4.5094e-03, -2.4952e-02, -5.7642e-02,  1.2502e-02],\n",
            "        [ 1.4318e-02,  1.7048e-01,  9.8645e-02,  1.1626e-01, -6.4602e-03,\n",
            "          3.6475e-03, -2.6616e-03,  1.8463e-02,  7.4380e-02,  4.8675e-03],\n",
            "        [-3.3771e-03,  7.5635e-01,  4.9231e-01,  5.9236e-01, -2.6535e-02,\n",
            "         -3.3399e-04,  9.2968e-03,  1.8609e-01,  2.8954e-01,  1.1638e-02],\n",
            "        [ 1.6947e-02,  3.6135e-01,  2.2586e-01,  2.6266e-01, -1.4498e-02,\n",
            "         -1.4728e-02,  7.6450e-04,  8.2541e-02,  1.4173e-01, -4.6492e-03],\n",
            "        [-7.3543e-04, -6.6392e-03, -9.4740e-03, -9.7109e-03, -7.2410e-03,\n",
            "         -1.1080e-02,  1.3073e-02, -1.5281e-03, -1.2739e-02,  8.3664e-03],\n",
            "        [ 6.0610e-03, -1.2677e-02, -7.7717e-03,  3.0218e-03,  8.7225e-04,\n",
            "         -1.2090e-02,  1.6669e-02, -2.5869e-03, -2.0800e-03,  1.1837e-02],\n",
            "        [-8.6392e-03, -2.1394e-03, -1.0249e-02, -7.1736e-03, -6.2326e-04,\n",
            "          1.7110e-03, -1.7181e-02,  4.3232e-03, -4.0515e-04, -4.3016e-03]])\n",
            "4.bias tensor([-3.6226e-03, -2.8736e-03, -2.5085e-04,  1.3606e-01, -5.4787e-03,\n",
            "        -2.1939e-02, -1.2181e-02,  0.0000e+00, -7.5774e-05, -6.1580e-04])\n",
            "6.weight tensor([[-0.0181, -0.0144,  0.0050, -0.2491,  0.2221,  1.0499,  0.4881, -0.0125,\n",
            "          0.0020,  0.0123]])\n",
            "6.bias tensor([0.0338])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "#torch.save(model.state_dict(),\"xor.pt\")\n",
        "\n",
        "#THIS CODE IS USED FOR GPU ACCELERATION\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of XOR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
